{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Algorithm\n",
    "\n",
    "* Last class, we introduced the probabilistic generative classifier.  \n",
    "* As discussed, the probabilistic generative classifier requires us to assume a parametric form for each class (e.g., each class is represented by a multi-variate Gaussian distribution, etc..).  Because of this, the probabilistic generative classifier is a *parametric* approach.\n",
    "* Parametric approaches have the drawback that the functional parametric form needs to be decided/assumed in advance and, if chosen poorly, might be a poor model of the distribution that generates the data resulting in poor performance. \n",
    "* Non-parametric methods are those that do not assume a particular generating distribution for the data.  The $K$-nearest neighbors algorithm is one example of a non-parametric classifier. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nearest neighbor methods compare a test point to the $k$ nearest training data points and then estimate an output value based on the desired/true output values of the $k$ nearest training points\n",
    "* Essentially, there is no ''training'' other than storing the training data points and their desired outputs\n",
    "* In test, you need to: (1) determine which $k$ training data points are closest to the test point; and (2) determine the output value for the test point \n",
    "* In order to find the $k$ nearest neighbors in the training data, you need to define a *similarity measure* or a *dissimilarity measure*.  The most common dissimilarity measure is Euclidean distance. \n",
    "    * Euclidean distance: $d_E = \\sqrt{\\left(\\mathbf{x}_1-\\mathbf{x}_2\\right)^T\\left(\\mathbf{x}_1-\\mathbf{x}_2\\right)}$\n",
    "    * City block distance: $d_C = \\sum_{i=1}^d \\left| x_{1i} - x_{2i} \\right|$\n",
    "    * Mahalanobis distance: $\\left(\\mathbf{x}_1-\\mathbf{x}_2\\right)^T\\Sigma^{-1}\\left(\\mathbf{x}_1-\\mathbf{x}_2\\right)$\n",
    "    * Cosine angle similarity: $\\cos \\theta = \\frac{\\mathbf{x}_1^T\\mathbf{x}_2}{\\left\\|\\mathbf{x}_1\\right\\|_2^2\\left\\|\\mathbf{x}_2\\right\\|_2^2}$\n",
    "    * and many more...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you are doing classification, once you find the $k$ nearest neighbors to your test point in the training data, then you can determine the class label of your test point using (most commonly) *majority vote*\n",
    "* If there are ties, they can be broken randomly or using schemes like applying the label of the closest data point in the neighborhood\n",
    "*  Of course, there are MANY modifications you can make to this.  A common one is to weight the votes of each of the nearest neighbors by their distance/similarity measure value.  If they are closer, they get more weight. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 6, n_neighbors = 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0289129359a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Plot the decision boundary. For that, we will assign a color to each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/neighbors/classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;34m\"Expected n_neighbors <= n_samples, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;34m\" but n_samples = %d, n_neighbors = %d\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m             )\n\u001b[1;32m    349\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 6, n_neighbors = 10"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1224x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reference for some code: http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn import neighbors\n",
    "%matplotlib inline \n",
    "\n",
    "#figure params\n",
    "h = .02  # step size in the mesh\n",
    "figure = plt.figure(figsize=(17, 9))\n",
    "\n",
    "#set up classifiers\n",
    "n_neighbors = 10\n",
    "classifiers = []\n",
    "classifiers.append(neighbors.KNeighborsClassifier(n_neighbors, weights='uniform'))\n",
    "classifiers.append(neighbors.KNeighborsClassifier(n_neighbors, weights='distance'))\n",
    "names = ['K-NN_Uniform', 'K-NN_Weighted']\n",
    "\n",
    "#Put together datasets\n",
    "n_samples = 30\n",
    "X, y = make_classification(n_samples, n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=0, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(n_samples, noise=0.3, random_state=0),\n",
    "            make_circles(n_samples,  noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable]\n",
    "\n",
    "\n",
    "\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for X, y in datasets:\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.8)  #split into train/test folds\n",
    "\n",
    "    #set up meshgrid for figure\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "    # and testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], marker='+', c=y_test, cmap=cm_bright, alpha=0.6)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot also the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], marker='+', c=y_test, cmap=cm_bright,\n",
    "                   alpha=0.4)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "figure.subplots_adjust(left=.02, right=.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Error and Evaluation Metrics\n",
    "\n",
    "* A key step in machine learning algorithm development and testing is determining a good error and evaluation metric. \n",
    "\n",
    "* Evaluation metrics help us to estimate how well our model is trained and it is important to pick a metric that matches our overall goal for the system.  \n",
    "\n",
    "* Some common evaluation metrics include precision, recall, receiver operating curves, and confusion matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classification Accuracy and Error \n",
    "\n",
    "* Classification accuracy is defined as the number of correctly classified samples divided by all samples:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{accuracy} = \\frac{N_{cor}}{N} \n",
    "\\end{equation}\n",
    "where $N_{cor}$ is the number of correct classified samples and $N$ is the total number of samples.\n",
    "\n",
    "* Classification error is defined as the number of incorrectly classified samples divided by all samples:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{error} = \\frac{N_{mis}}{N}\n",
    "\\end{equation}\n",
    "where $N_{mis}$ is the number of misclassified samples and $N$ is the total number of samples.\n",
    "\n",
    "* Suppose there is a 3-class classification problem, in which we would like to classify each training sample (a fish) to one of the three classes (A = salmon or B = sea bass or C = cod). \n",
    "\n",
    "* Let's assume there are 150 samples, including 50 salmon, 50 sea bass and 50 cod.  Suppose our model misclassifies 3 salmon, 2 sea bass and 4 cod.\n",
    "\n",
    "* Prediction accuracy of our binary classification model is calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{accuracy} = \\frac{47+48+46}{50+50+50} = \\frac{47}{50}\n",
    "\\end{equation}\n",
    "\n",
    "* Prediction error is calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{error} = \\frac{N_{mis}}{N} = \\frac{3+2+4}{50+50+50} = \\frac{3}{50}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices\n",
    "\n",
    "* A confusion matrix summarizes the classification accuracy across several classes. It shows the ways in which our classification model is confused when it makes predictions, allowing visualization of the performance of our algorithm. Generally, each row represents the instances of an actual class while each column represents the instances in a predicted class. \n",
    "\n",
    "* If our classifier is trained to distinguish between salmon, sea bass and cod, then we can summarize the prediction result in the confusion matrix as follows:\n",
    "    \n",
    "\n",
    "| Actual/Predicted | Salmon | Sea bass | Cod  |\n",
    "| --- | --- | --- | --- |\n",
    "| Salmon | 47 | 2 | 1 |\n",
    "| Sea Bass | 2 | 48 | 0 |\n",
    "| Cod | 0 | 0 | 50 |\n",
    "\n",
    "* In this confusion matrix, of the 50 actual salmon, the classifier predicted that 2 are sea bass, 1 is cod incorrectly and 47 are labeled salmon correctly. All correct predictions are located in the diagonal of the table. So it is easy to visually inspect the table for prediction errors, as they will be represented by values outside the diagonal. \n",
    "\n",
    "### TP, FP, TN, and FN\n",
    "\n",
    "* True positive (TP): correctly predicting event values\n",
    "* False positive (FP): incorrectly calling non-events as an event\n",
    "* True negative (TN): correctly predicting non-event values\n",
    "* False negative (FN): incorrectly labeling events as non-event\n",
    "\n",
    "\n",
    "* Precision is also called positive predictive value.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}\n",
    "\\end{equation}\n",
    "\n",
    "* Recall is also called true positive rate, probability of detection\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "* Fall-out is also called false positive rate, probability of false alarm.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Fall-out} = \\frac{\\text{FP}}{\\text{N}}= \\frac{\\text{FP}}{\\text{FP}+\\text{TN}}\n",
    "\\end{equation}\n",
    "\n",
    "* *Consider the salmon/non-salmon classification problem, what are the TP, FP, TN, FN values?*\n",
    "\n",
    "| Actual/Predicted | Salmon | Non-Salmon  |\n",
    "| --- | --- | --- | \n",
    "| Salmon | 47 | 3 | \n",
    "| Non-Salmon | 2 | 98 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curves \n",
    "\n",
    "* The Receiver Operating Characteristic (ROC) curve is a plot between the true positive rate (TPR) and the false positive rate (FPR), where the TPR is defined on the $y$-axis and FPR is defined on the $x$-axis. \n",
    "\n",
    "* $TPR = TP/(TP+FN)$ is defined as ratio between true positive prediction and all real positive samples. The definition used for $FPR$ in a ROC curve is often problem dependent.  For example, for detection of targets in an area, FPR may be defined as the ratio between the number of false alarms per unit area ($FA/m^2$).  In another example, if you have a set number of images and you are looking for targets in these collection of images, FPR may be defined as the number of false alarms per image.  In some cases, it may make the most sense to simply use the Fall-out or false positive rate.\n",
    "\n",
    "* Given a binary classifier and its threshold, the (x,y) coordinates of ROC space can be calculated from all the prediction result.  You trace out a ROC curve by varying the threshold to get all of the points on the ROC.\n",
    "\n",
    "* The diagonal between (0,0) and (1,1) separates the ROC space into two areas, which are left up area and right bottom area. The points above the diagonal represent good classification (better than random guess) which below the diagonal represent bad classification (worse than random guess).\n",
    "\n",
    "* *What is the perfect prediction point in a ROC curve?*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE and MAE\n",
    "\n",
    "\n",
    "* *Mean Square Error* (MSE) is the average of the squared error between prediction and actual observation. \n",
    "\n",
    "* For each sample $\\mathbf{x}_i$, the prediction value is $y_i$ and the actual output is $d_i$. The MSE is\n",
    "\n",
    "\\begin{equation}\n",
    "MSE = \\sum_{i=1}^n \\frac{(d_i - y_i)^2}{n}\n",
    "\\end{equation}\n",
    "\n",
    "* *Root Mean Square Error* (RMSE) is simply the square root the MSE. \n",
    "\n",
    "\\begin{equation}\n",
    "RMSE = \\sqrt{MSE}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "* *Mean Absolute Error* (MAE) is the average of the absolute error.\n",
    "\\begin{equation}\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^n \\lvert d_i - y_i \\rvert\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
