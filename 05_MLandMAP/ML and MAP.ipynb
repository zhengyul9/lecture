{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood and Maximum A Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We looked at the regularization term as a *penalty* term in the objective function.  There is another way to interpret the regularization term as well.  Specifically, there is a *Bayesian* interpretation. \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\min E^{\\ast}(\\mathbf{w}) &=& \\max -E^{\\ast}(\\mathbf{w})\\\\\n",
    "& =& \\max \\exp \\left\\{ -E^{\\ast}(\\mathbf{w})\\right\\}\\\\\n",
    "&=& \\max \\exp \\left\\{ -\\frac{1}{2}\\sum_{n=1}^N \\left( y(x_n, \\mathbf{w}) - t_n \\right)^2 - \\frac{\\lambda}{2}\\left\\| \\mathbf{w} \\right\\|^2_2 \\right\\}\\\\\n",
    "&=& \\max \\exp \\left\\{ -\\frac{1}{2}\\sum_{n=1}^N \\left( y(x_n, \\mathbf{w}) - t_n \\right)^2 \\right\\}\\exp\\left\\{-\\frac{1}{2}\\lambda\\left\\| \\mathbf{w} \\right\\|^2_2\\right\\}\\\\\n",
    "&=& \\max \\prod_{n=1}^N \\exp \\left\\{ -\\frac{1}{2} \\left( y(x_n, \\mathbf{w}) - t_n \\right)^2 \\right\\}\\exp\\left\\{-\\frac{1}{2}\\lambda\\left\\| \\mathbf{w} \\right\\|^2_2\\right\\}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So, this is a maximization of the *data likelihood* with a *prior*: $p(\\mathbf{X}|\\mathbf{w})p(\\mathbf{w})$\n",
    "\n",
    "* *Method of Maximum Likelihood:*\n",
    "    * A *data likelihood* is how likely the data is given the parameter set\n",
    "    * So, if we want to maximize how likely the data is to have come from the model we fit, we should find the parameters that maximize the likelihood\n",
    "    * A common trick to maximizing the likelihood is to maximize the log likelihood.  Often makes the math much easier.  *Why can we maximize the log likelihood instead of the likelihood and still get the same answer?*\n",
    "    * Consider: $\\max \\ln \\exp \\left\\{ -\\frac{1}{2}\\left(y(x_n, \\mathbf{w}) - t_n\\right)^2\\right\\}$ We go back to our original objective. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Method of Maximum A Posteriori (MAP):*\n",
    "    * Bayes Rule: $p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)}$\n",
    "    * Consider: $p(\\mathbf{w}|\\mathscr{D}) = \\frac{p(\\mathscr{D}|\\mathbf{w})p(\\mathbf{w})}{p(\\mathscr{D})}$, i.e., posterior $\\propto$ likelihood $\\times$ prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood vs. Maximum A Posteriori (MAP)\n",
    "\n",
    "* Lets look at this in terms of binary variables, e.g., Flipping a coin:  $X =1$ is heads, $X=0$ is tails\n",
    "* Let $\\mu$ be the probability of heads.  If we know $\\mu$, then: $P(x = 1 |\\mu) = \\mu$ and $P(x = 0|\\mu) = 1-\\mu$\n",
    "\\begin{eqnarray}\n",
    "P(x|\\mu) = \\mu^x(1-\\mu)^{1-x} = \\left\\{\\begin{array}{c c}\\mu & \\text{ if } x=1 \\\\ 1-\\mu & \\text{ if } x = 0 \\end{array}\\right.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is called the *Bernoulli* distribution.  The mean and variance of a Bernoulli distribution is: \n",
    "\\begin{equation}\n",
    "E[x] = \\mu\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "E\\left[(x-\\mu)^2\\right] = \\mu(1-\\mu)\n",
    "\\end{equation}\n",
    "* So, suppose we conducted many Bernoulli trials (e.g., coin flips) and we want to estimate $\\mu$\n",
    "\n",
    "### Method: Maximum Likelihood\n",
    "\\begin{eqnarray}\n",
    "p(\\mathscr{D}|\\mu) &=& \\prod_{n=1}^N p(x_n|\\mu) \\\\\n",
    "&=& \\prod_{n=1}^N \\mu^{x_n}(1-\\mu)^{1-x_n}\n",
    "\\end{eqnarray}\n",
    "\n",
    "* Maximize : (*What trick should we use?*)\n",
    "\\begin{eqnarray}\n",
    "\\mathscr{L} = \\sum_{n=1}^N x_n \\ln \\mu + (1-x_n)\\ln(1-\\mu)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mu} =  0 &=& \\frac{1}{\\mu}\\sum_{n=1}^N x_n - \\frac{1}{1-\\mu }\\sum_{n=1}^N (1 - x_n)\\\\\n",
    "0 &=& \\frac{(1-\\mu) \\sum_{n=1}^N x_n - \\mu \\sum_{n=1}^N (1- x_n)}{\\mu(1-\\mu)}\\\\\n",
    "0 &=& \\sum_{n=1}^N x_n - \\mu \\sum_{n=1}^N x_n - \\mu \\sum_{n=1}^N 1 + \\mu \\sum_{n=1}^N x_n\\\\\n",
    "0 &=& \\sum_{n=1}^N x_n - \\mu N\\\\\n",
    "\\mu &=& \\frac{1}{N}\\sum_{n=1}^N x_n = \\frac{m}{N}\n",
    "\\end{eqnarray}\n",
    "where $m$ is the number of successful trials. \n",
    "\n",
    "* So, if we flip a coin 1 time and get heads, then $\\mu = 1$ and probability of getting tails is 0.  *Would you believe that? We need a prior!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method: Maximum A Posteriori: \n",
    "\n",
    "* Look at several independent trials.  Consider N = 3 and m = 2 (N is number of trials, m is number of successes) and look at all ways to get 2 H and 1 T: \n",
    "     * H H T  $\\rightarrow \\mu \\mu (1-\\mu) = \\mu^2(1-\\mu)$\n",
    "     * H T H  $\\rightarrow \\mu  (1-\\mu) \\mu  = \\mu^2(1-\\mu)$\n",
    "     * T H H $\\rightarrow (1-\\mu) \\mu   \\mu  = \\mu^2(1-\\mu)$\n",
    "\n",
    "* $\\left(\\begin{array}{c} 3 \\\\ 2 \\end{array}\\right) \\mu^2(1-\\mu) \\rightarrow \\left(\\begin{array}{c} N \\\\ m \\end{array}\\right) \\mu^m(1-\\mu)^{N-m} = \\frac{N!}{(N-m)!m!}\\mu^m(1-\\mu)^{N-m} $\n",
    "* This is the Binomial Distribution, gives the probability of $m$ observations of $x=1$ out of N independent trails\n",
    "* So, what we saw is that we need a prior.  We want to incorporate our prior belief. Let us place a prior on $\\mu$\n",
    "\\begin{equation}\n",
    "Beta(\\mu|a,b) = \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)}\\mu^{a-1}(1-\\mu)^{b-1}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "E[\\mu] = \\frac{a}{a + b}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "Var[\\mu] = \\frac{ab}{(a+b)^2(a+b+1)}\n",
    "\\end{equation}\n",
    "* Note: $\\Gamma(x) = \\int_0^\\infty u^{x-1}e^{-u} du$ and when $x$ is an integer, then it simplifies to $(x-1)!$\n",
    "* Calculation of the posterior, Take $N = m + l$ observations:\n",
    "\\begin{eqnarray}\n",
    "p(\\mu | m, l, a, b) &\\propto& Bin(m,l|\\mu)Beta(\\mu|a,b) \\\\\n",
    "&\\propto& \\mu^m(1-\\mu)^l\\mu^{a-1}(1-\\mu)^{b-1}\\\\\n",
    "&=& \\mu^{m+a-1}(1-\\mu)^{l+b-1}\n",
    "\\end{eqnarray}\n",
    "* What does this look like?  Beta: $a \\leftarrow m+a$, $b \\leftarrow l+b$\n",
    "* So, what's the posterior? \n",
    "\\begin{equation}\n",
    "p(\\mu | m, l, a, b) = \\frac{\\Gamma(m+a+l+b)}{\\Gamma(m+a)\\Gamma(l+b)}\\mu^{m+a-1}(1-\\mu)^{l+b-1}\n",
    "\\end{equation}\n",
    "* *Conjugate Prior Relationship:* When the posterior is the same form as the prior\n",
    "* Now we can maximize the (log of the) posterior: \n",
    "\\begin{eqnarray}\n",
    "\\max_\\mu ((m+a-1) \\ln \\mu + (l+b-1) \\ln (1-\\mu))\n",
    "\\end{eqnarray}\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mu} =  0&=& \\frac{m + a -1}{\\mu} - \\frac{l + b - 1}{1-\\mu}\\\\\n",
    "&=& (1-\\mu)(m+a-1) - \\mu(l+b-1)\\\\\n",
    "&=& (m+a-1) - \\mu(m+a-1) - \\mu(l+b-1)\\\\\n",
    "\\mu &=& \\frac{m+a-1}{m+a+l+b-2}\n",
    "\\end{eqnarray}\n",
    "* This is the MAP solution.  *So, what happens now when you flip one heads, two heads, etc.?*\n",
    "* Discuss online updating of the prior.  Eventually the data takes over the prior. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "%matplotlib inline   \n",
    "\n",
    "priorA = 2\n",
    "priorB = 2\n",
    "def plotBeta(a=priorA,b=priorB):\n",
    "\t'''plotBeta(a=1,b=1): Plot plot beta distribution with parameters a and b'''\n",
    "\txrange = np.arange(0,1,0.001)  #get equally spaced points in the xrange\n",
    "\tnormconst = math.gamma(a+b)/(math.gamma(a)*math.gamma(b))\n",
    "\tbeta = normconst*xrange**(a-1)*(1-xrange)**(b-1)\n",
    "\tfig = plt.figure()\n",
    "\tp1 = plt.plot(xrange,beta, 'g')\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "#Beta Distribution\n",
    "# plotBeta(priorA,priorB);\n",
    "\n",
    "trueMu = 0.5\n",
    "numFlips = 100\n",
    "flipResult = []\n",
    "for flip in range(numFlips):\n",
    "    flipResult.append(np.random.binomial(1,trueMu,1)[0])\n",
    "    print(flipResult)\n",
    "    print('Frequentist/Maximum Likelihood Probability of Heads:' + str(sum(flipResult)/len(flipResult)))\n",
    "    print('Bayesian/MAP Probability of Heads:' + str((sum(flipResult)+priorA-1)/(len(flipResult)+priorA+priorB-2)))\n",
    "    if (input(\"Hit enter to continue, or q to quit...\\n\") == \"q\"):\n",
    "        print(\"quitting...\\n\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gaussian Distribution:\n",
    "* Consider a univariate Gaussian distribution:\n",
    "\\begin{equation}\n",
    "\\mathscr{N}(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2} \\right\\}\n",
    "\\end{equation}\n",
    "* $\\sigma^2$ is the variance OR $\\frac{1}{\\sigma^2}$ is the *precision*\n",
    "* So, as $\\lambda$ gets big, variance gets smaller/tighter.  As $\\lambda$ gets small, variance gets larger/wider.\n",
    "* The Gaussian distribution is also called the *Normal* distribution. \n",
    "* We will often write $N(x|\\mu, \\sigma^2)$ to refer to a Gaussian with mean $\\mu$ and variance $\\sigma^2$.\n",
    "* *What is the multi-variate Gaussian distribution?* \n",
    "\n",
    "* What is the expected value of $x$ for the Gaussian distribution?\n",
    "\\begin{eqnarray}\n",
    "E[x] &=& \\int x p(x) dx \\\\\n",
    "     &=& \\int x \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2} \\right\\} dx\n",
    "\\end{eqnarray}\n",
    "* *Change of variables:*  Let\n",
    "\\begin{eqnarray}\n",
    "y &=& \\frac{x-\\mu}{\\sigma} \\rightarrow x = \\sigma y + \\mu\\\\\n",
    "dy &=& \\frac{1}{\\sigma} dx \\rightarrow dx = \\sigma dy\n",
    "\\end{eqnarray}\n",
    "* Plugging this into the expectation: \n",
    "\\begin{eqnarray}\n",
    "E[x] &=& \\int \\left(\\sigma y + \\mu  \\right)\\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left\\{ - \\frac{1}{2} y^2 \\right\\} \\sigma dy \\\\\n",
    "&=& \\int \\frac{\\sigma y}{\\sqrt{2\\pi}} \\exp\\left\\{ - \\frac{1}{2} y^2 \\right\\} dy + \\int \\frac{\\mu}{\\sqrt{2\\pi}} \\exp\\left\\{ - \\frac{1}{2} y^2 \\right\\} dy \n",
    "\\end{eqnarray}\n",
    "* The first term is an odd function: $f(-y) = -f(y)$  So, $E[x] = 0 + \\mu = \\mu$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MLE of Mean of Gaussian\n",
    "\n",
    "*  Let $\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_N$ be samples from a multi-variance Normal distribution with known covariance matrix and an unknown mean.  Given this data, obtain the ML estimate of the mean vector. \n",
    "\t\\begin{equation}\n",
    "\tp(\\mathbf{x}_k| {{\\mu}}) = \\frac{1}{(2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}}}\\exp\\left( -\\frac{1}{2}(\\mathbf{x}_k - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_k - {\\mu})\\right)\n",
    "\t\\end{equation}\n",
    "* We can define our likelihood given the $N$ data points.  We are assuming these data points are drawn independently but from an identical distribution (i.i.d.):\n",
    "\t\\begin{equation}\n",
    "\t\\prod_{n=1}^N p(\\mathbf{x}_n| {{\\mu}}) = \\prod_{n=1}^N \\frac{1}{(2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}}}\\exp\\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu})\\right)\n",
    "\t\\end{equation}\n",
    "*  We can apply our \"trick\" to simplify\n",
    "\t\\begin{eqnarray}\n",
    "\t\\mathscr{L} &=& \\ln \\prod_{n=1}^N p(\\mathbf{x}_n| {{\\mu}}) = \\ln \\prod_{n=1}^N \\frac{1}{(2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}}}\\exp\\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu})\\right)\\\\\n",
    "\t&=& \\sum_{n=1}^N  \\ln \\frac{1}{(2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}}}\\exp\\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu})\\right)\\\\\n",
    "\t&=& \\sum_{n=1}^N  \\left( \\ln \\frac{1}{(2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}}} + \\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu})\\right) \\right) \\\\\n",
    "\t&=&  - N \\ln (2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}} + \\sum_{n=1}^N  \\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu}) \\right) \n",
    "\t\\end{eqnarray}\n",
    "* Now, lets maximize:\n",
    "\t\\begin{eqnarray}\n",
    "\t\\frac{\\partial \\mathscr{L}}{\\partial \\mu} &=& \\frac{\\partial}{\\partial \\mu} \\left[- N \\ln (2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}} + \\sum_{n=1}^N  \\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu}) \\right)\\right] = 0 \\\\\n",
    "\t&\\rightarrow& \\sum_{n=1}^N  \\Sigma^{-1}(\\mathbf{x}_n - {\\mu}) = 0\\\\\n",
    "\t&\\rightarrow& \\sum_{n=1}^N  \\Sigma^{-1}\\mathbf{x}_n  = \\sum_{n=1}^N  \\Sigma^{-1} {\\mu}\\\\\n",
    "\t&\\rightarrow& \\Sigma^{-1} \\sum_{n=1}^N \\mathbf{x}_n  = \\Sigma^{-1} {\\mu} N\\\\\n",
    "\t&\\rightarrow& \\sum_{n=1}^N \\mathbf{x}_n  = {\\mu} N\\\\\n",
    "\t&\\rightarrow& \\frac{\\sum_{n=1}^N \\mathbf{x}_n}{N} = {\\mu}\\\\\n",
    "\t\\end{eqnarray}\n",
    "* So, the ML estimate of $\\mu$ is the sample mean!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP of Mean of Gaussian\n",
    "\n",
    "* To get a MAP estimate of the mean of a Gaussian, we apply a prior distribution and maximize the posterior. \n",
    "* Lets use a Gaussian prior on the mean (because it has a *conjugate prior* relationship)\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(\\mu|X, \\mu_0, \\sigma_0^2, \\sigma^2) &\\propto& \\mathscr{N}(X|\\mu, \\sigma^2)\\mathscr{N}(\\mu|\\mu_0, \\sigma_0^2)\\\\\n",
    "&=& \\prod_{n=1}^N \\left(\\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left\\{-\\frac{1}{2\\sigma^2}\\left(x_n-\\mu\\right)^2 \\right\\}\\right)\\frac{1}{\\sqrt{2\\pi \\sigma_0^2}} \\exp\\left\\{-\\frac{1}{2\\sigma_0^2}\\left(\\mu-\\mu_0\\right)^2 \\right\\}\\nonumber\\\\\n",
    "\\mathscr{L} &=& -\\frac{N}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{n=1}^N(x_n-\\mu)^2 - \\frac{N}{2}\\ln(2\\pi \\sigma_0^2) - \\frac{N}{2\\sigma_0^2}(\\mu - \\mu_0)^2\\\\\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mu} &=&  - \\frac{N}{\\sigma^2}\\mu - \\frac{N}{\\sigma_0^2}\\mu + \\frac{N}{\\sigma_0^2}\\mu_0 + \\frac{1}{\\sigma^2}\\sum_{n=1}^N x_n = 0\\\\\n",
    "N\\mu\\left(\\frac{\\sigma_0^2 + \\sigma^2}{\\sigma^2\\sigma_0^2} \\right) &=& \\frac{1}{\\sigma^2}\\sum_{n=1}^N x_n + \\frac{1}{\\sigma_0^2}\\mu_0 \\\\\n",
    "\\mu_{MAP} &=& \\frac{\\sigma_0^2}{N\\sigma_0^2 + N\\sigma^2}\\sum_{n=1}^N x_n + \\frac{\\mu_0\\sigma^2}{N\\sigma_0^2 + N\\sigma^2}\n",
    "\\end{eqnarray}\n",
    "\n",
    "* *Does this result make sense?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
