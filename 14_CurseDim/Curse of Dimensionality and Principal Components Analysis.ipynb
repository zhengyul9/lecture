{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The number of points needed to densely populate a space grows quickly with dimensionality. If you add informative, discriminating features - more features can be helpful - but you will need more and more points to fully understand the space.\n",
    "\n",
    "* Things do not always behave as you would expect in high dimensions - this is part of the Curse of Dimensionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Porcupine:  The unit hyper-sphere inscribed within the unit hyper-cube.\n",
    "\n",
    "* Consider a sphere with radius $r$ in $D$ dimensions\n",
    "\\begin{equation}\n",
    "S = \\left\\{ \\mathbf{x} \\left| \\sum_{i=1}^D x_i^2 \\le r^2 \\right. \\right\\} \\nonumber\n",
    "\\end{equation}\n",
    "It's volume is: \n",
    "\\begin{equation}\n",
    "v_D(r) = \\frac{r^D \\pi^{\\frac{D}{2}}}{\\Gamma(\\frac{D}{2} + 1)} \\nonumber\n",
    "\\end{equation}\n",
    "where $\\Gamma(n) = \\int_0^\\infty e^{-x}x^{n-1}dx$.\n",
    "\n",
    "  So, for $D = 1$: $v_1(r) = \\frac{r \\pi^{1/2}}{\\Gamma(1/2 + 1)} = 2r$ \n",
    "  \n",
    "  $D = 2$: $v_2(r) = \\frac{r^2 \\pi}{\\Gamma(2)} = \\pi r^2$\n",
    "  \n",
    "   $D = 3$: $v_3(r) = \\frac{r^2 \\pi^{3/2}}{\\Gamma(3/2 + 1)} = \\frac{4}{3}\\pi r^3$\n",
    "    \n",
    "* Consider a hypercube with radius $r$.  It's volume is $(2r)^D$.\n",
    "\n",
    "So, for $D = 1$: $v_{1,c} = 2r$ \n",
    "\n",
    "$D = 2$: $v_{2,c} = 4r^2$ \n",
    "\n",
    "$D = 3$: $v_{3,c} = 8r^3$ \n",
    "* Take the case where the hyper-sphere is inscribed within the unit hyper-cube.  What happens to the relative volume of the sphere and cube as $D$ increases? \n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{Vol(Sphere)}{Vol(Cube)} &=& \\frac{r^D\\pi^{\\frac{D}{2}}}{\\Gamma(\\frac{D}{2} +1)(2r)^D}\\\\ \\nonumber\n",
    "&=& \\frac{\\pi^{\\frac{D}{2}}}{2^D\\Gamma(\\frac{D}{2} + 1)}\n",
    "\\end{eqnarray}\n",
    "Note: The $r$ dropped out, relative volume depends only on dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed python libraries\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import math \n",
    "\n",
    "#The Unit Porcupine Plot\n",
    "#The unit hyper-sphere inscribed within the unit hyper-cube.\n",
    "\n",
    "fig = plt.figure()\n",
    "d = range(1,100)\n",
    "V = [math.pi**(i/2)/(2**i*math.gamma(i/2 + 1)) for i in d]\n",
    "ax = fig.add_subplot(*[1,2,1])\n",
    "ax.plot(d, V) \n",
    "ax.set_title('Ratio of Volume')\n",
    "\n",
    "dCorner = [math.sqrt(d) for d in range(1,10000)]\n",
    "ax = fig.add_subplot(*[1,2,2])\n",
    "ax.plot(range(1,10000), dCorner) \n",
    "ax.set_title('Distance to Corner')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volume of space between two spheres with slightly different radii in high dimensions\n",
    "\n",
    "* $Vol_{crust} = Vol_{S_1} - Vol_{S_2}$ where radius of $S_1$ is greater than the radius of $S_2$\n",
    "\\begin{eqnarray}\n",
    "Vol_{crust} &=& Vol_{S_1} - Vol_{S_2} \\nonumber\\\\\n",
    "&=& \\left[ 1 - \\frac{Vol_{S_2}}{Vol_{S_1}}\\right]Vol_{S_1} \\nonumber \\\\\n",
    "&=& \\left[ 1 - \\frac{\\frac{(a - \\epsilon)^D \\pi^{\\frac{D}{2}}}{\\Gamma(\\frac{D}{2}+1)}}{\\frac{a^D \\pi^{\\frac{D}{2}}}{\\Gamma(\\frac{D}{2}+1)}}\\right]Vol_{S_1} \\nonumber \\\\\n",
    "&=& \\left[ 1 - \\frac{a^D ( 1 - \\frac{\\epsilon}{a})^D}{a^D}\\right]Vol_{S_1}   \\nonumber \\\\\n",
    "&=&  \\left[ 1 - \\left( 1 - \\frac{\\epsilon}{a}\\right)^D \\right]Vol_{S_1}\\nonumber \n",
    "\\end{eqnarray}\n",
    "\n",
    "* What happens as $D$ increases?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crust volume between spheres with epsilon different radii and increasing dimensionality\n",
    "a = 1\n",
    "eps = 0.001\n",
    "D = np.arange(1,10000)\n",
    "RatioVol = [1-(1-eps/a)**d for d in D]\n",
    "fig = plt.figure()\n",
    "plt.plot(D, RatioVol)\n",
    "plt.title('Ratio of Volume of Crust to Bigger Sphere')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radius needed to cover the same percentage volume with growing dimensionality:  \n",
    "\n",
    "* Volume of unit line, square, cube, hyper-cube:  $s^D = 1^D$\n",
    "* Side of a cube covering some percentage of the area: say, 10% would be $r^D = 1/10$, $r = (1/10)^{(1/D)}$\n",
    "* What happens as D increases? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Illustrate that average distance between uniform random points increases when dimensionality\n",
    "increases and number of data points held constant. Illustrates that you need more and more\n",
    "points to characterize the unit cube as you go up in dimensionality.'''\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.spatial.distance as sc \n",
    "import textwrap\n",
    "\n",
    "N = 100\n",
    "\n",
    "x1 = np.random.uniform(0,1,(N,1)) #generate N uniform random data points in the unit interval\n",
    "x2 = np.random.uniform(0,1,(N,2)) #generate N uniform random data points in the unit square\n",
    "x3 = np.random.uniform(0,1,(N,3)) #generate N uniform random data points in the unit cube\n",
    "y1 = np.mean(sc.pdist(x1, 'euclidean')) #compute avg euclidean distance between points\n",
    "y2 = np.mean(sc.pdist(x2, 'euclidean'))\n",
    "y3 = np.mean(sc.pdist(x3, 'euclidean'))\n",
    "z1 = np.mean(sc.pdist(x1, 'cityblock')) #compute l1 distance between points\n",
    "z2 = np.mean(sc.pdist(x2, 'cityblock'))\n",
    "z3 = np.mean(sc.pdist(x3, 'cityblock'))\n",
    "\n",
    "#plot results\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(*[1,3,1])\n",
    "ax.scatter(x1, np.zeros((N,1)))\n",
    "myTitle = 'Avg. L2 Dist.: ' + str(\"%.2f\"%y1) + ' Avg. L1 Dist.: ' + str(\"%.2f\"%z1);\n",
    "ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 20)))\n",
    "\n",
    "ax = fig.add_subplot(*[1,3,2])\n",
    "ax.scatter(x2[:,0], x2[:,1]) \n",
    "myTitle = 'Avg. L2 Dist.: ' + str(\"%.2f\"%y2) + ' Avg. L1 Dist.: ' + str(\"%.2f\"%z2);\n",
    "ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 20)))\n",
    "\n",
    "ax = fig.add_subplot(*[1,3,3], projection='3d')\n",
    "ax.scatter(x3[:,0], x3[:,1], x3[:,2]) \n",
    "myTitle = 'Avg. L2 Dist.: ' + str(\"%.2f\"%y3) + ' Avg. L1 Dist.: ' + str(\"%.2f\"%z3);\n",
    "ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 20)))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentage Volume Illustration (p=.1)\n",
    "\n",
    "p = 0.1\n",
    "fig = plt.figure()\n",
    "x = np.arange(0,1,.05)\n",
    "\n",
    "# 1D \n",
    "ax = fig.add_subplot(*[1,3,1])\n",
    "ax.plot(x, np.zeros(x.size))\n",
    "z = np.arange(0,p,.01)\n",
    "ax.plot(z, np.zeros(z.size), 'r')\n",
    "\n",
    "# 2D \n",
    "ax = fig.add_subplot(*[1,3,2])\n",
    "[xx,yy] = np.meshgrid(x,x)\n",
    "z = np.arange(0,math.sqrt(p),0.01)\n",
    "[zx,zy] = np.meshgrid(z,z)\n",
    "ax.scatter(xx, yy)\n",
    "ax.scatter(zx, zy, color='red')\n",
    "\n",
    "# 3D \n",
    "ax = fig.add_subplot(*[1,3,3], projection='3d')\n",
    "[xx,yy,zz] = np.meshgrid(x,x,x)\n",
    "z = np.arange(0,math.sqrt(p),0.01)\n",
    "[zx,zy,zz2] = np.meshgrid(z,z,z)\n",
    "ax.scatter(xx, yy, zz)\n",
    "ax.scatter(zx, zy, zz2, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Analysis\n",
    "\n",
    "\n",
    "* So, a major take away from the Curse of Dimensionality discussion is that when we are in high dimensional spaces, much of the space is empty and the data lives at the surface.  Given this, it makes sense to use a lower-dimensional *manifold* representation of the data.  \n",
    "\n",
    "* A very common approach (and one of the simplest approaches) to dimensionality reduction is  Principal Components Analysis (PCA).  PCA  takes data from sensor coordinates to data centric coordinates using linear projections (i.e., it is assuming that the informative components of the data lies on a linear manifold.) \n",
    "\n",
    "* PCA uses a linear transformation to minimize the redundancy of the resulting transformed data (by ending up with data that is uncorrelated).\n",
    "\n",
    "* PCA finds the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one. \n",
    "\n",
    "* Without loss of generality, let's assume the input data has zero mean. \n",
    "\\begin{equation}\n",
    "\\mathbf{y} = \\mathbf{A}\\mathbf{x}\n",
    "\\end{equation}\n",
    "\n",
    "The correlation matrix of $\\mathbf{y}$ is: \n",
    "\\begin{eqnarray}\n",
    "R_y &=& E[\\mathbf{y}\\mathbf{y}^T]\\\\\n",
    "&=& E[\\mathbf{A}\\mathbf{x}\\mathbf{x}^T\\mathbf{A}^T]\\\\\n",
    "&=&\\mathbf{A}R_x\\mathbf{A}^T\n",
    "\\end{eqnarray}\n",
    "\n",
    " If we are given $N$ data vectors, $\\{ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\}$, we can estimate $R_x$ as \n",
    " \\begin{equation}\n",
    " R_x \\approx \\frac{1}{n} \\sum_{k=1}^n \\mathbf{x}_k \\mathbf{x}_k^T\n",
    " \\end{equation}\n",
    " * This is a symmetric matrix, so, it's eigenvectors are mutually orthogonal. \n",
    " * So, if we choose $\\mathbf{A}$ to have columns equal to the orthonormal eigenvectors of $R_x$, the $R_y$ is diagonal.  \n",
    " \\begin{eqnarray}\n",
    " \\mathbf{R}_y = \\left[ \\begin{array}{c} \\mathbf{e_1}^T \\\\ \\mathbf{e_2}^T \\\\ \\vdots \\\\ \\mathbf{e_D}^T\\end{array}\\right]\\mathbf{R}_x\\left[ \\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_D \\right] & = & \\left[ \\begin{array}{c c c c}\\mathbf{e}_1^T\\mathbf{R}_x\\mathbf{e}_1 & \\mathbf{e}_1^T\\mathbf{R}_x\\mathbf{e}_2 & \\ldots & \\mathbf{e}_1^T\\mathbf{R}_x\\mathbf{e}_D \\\\ \\mathbf{e}_2^T\\mathbf{R}_x\\mathbf{e}_1 & \\mathbf{e}_2^T\\mathbf{R}_x\\mathbf{e}_2 & \\ldots & \\mathbf{e}_2^T\\mathbf{R}_x\\mathbf{e}_D \\\\ \\vdots & & \\ddots & \\vdots \\\\ \\mathbf{e}_D^T\\mathbf{R}_x\\mathbf{e}_1 & \\mathbf{e}_D^T\\mathbf{R}_x\\mathbf{e}_D & \\ldots & \\mathbf{e}_D^T\\mathbf{R}_x\\mathbf{e}_D \\end{array}\\right]\\\\\n",
    " & = & \\left[ \\begin{array}{c c c c} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & & \\ddots & \\\\ 0 & 0 & \\cdots & \\lambda_D \\end{array}\\right]\n",
    "\\end{eqnarray}\n",
    "where $\\mathbf{e}_i \\in \\mathbb{R}^{D\\times1}$ and $\\mathbf{R}_x \\in \\mathbb{R}^{D\\times D}$. \n",
    "\n",
    "* Note: Given that $\\mathbf{e}_i$ is an eigenvector of $\\mathbf{R}_x$, we know that $\\lambda_i\\mathbf{e}_i = \\mathbf{R}_x\\mathbf{e}_i$.  So, $\\mathbf{e}_i^T\\mathbf{R}_x\\mathbf{e}_i = \\mathbf{e}_i^T\\left( \\lambda_i \\mathbf{e}_i \\right) = \\lambda_i$ using the fact that $\\mathbf{e}_i$ is normalized (i.e., $\\left\\|\\mathbf{e}_i\\right\\|_2^2$ = 1)\n",
    "\n",
    "* Similarly, Given that $\\mathbf{e}_i$ and $\\mathbf{e}_j$ are orthogonal eigenvectors of $\\mathbf{R}_x$, we know that $\\lambda_i\\mathbf{e}_i = \\mathbf{R}_x\\mathbf{e}_i$.  So, $\\mathbf{e}_j^T\\mathbf{R}_x\\mathbf{e}_i = \\mathbf{e}_j^T\\left( \\lambda_i \\mathbf{e}_i \\right) = 0$\n",
    "\n",
    "\n",
    "## Given a symmetric matrix, the eigenvectors of distinct eigenvalues are orthogonal:\n",
    "\n",
    "Let $\\mathbf{A}$ be an $l\\times l$ symmetric matrix, $\\mathbf{A}^T = \\mathbf{A}$.  Then the eigenvectors corresponding to distinct eigenvalues are orthogonal.  Let $\\lambda_i \\ne \\lambda_j$ be two such eigenvalues.  The definitions we have:\n",
    " \\begin{eqnarray}\n",
    " \\mathbf{A} \\mathbf{v}_i = \\lambda_i\\mathbf{v}_i \\\\\n",
    " \\mathbf{A} \\mathbf{v}_j = \\lambda_j\\mathbf{v}_j \n",
    " \\end{eqnarray}\n",
    " \n",
    " By multiplying the first eigenvalue equation on the left by $\\mathbf{v}_j^T$ and by the transpose of the second equation on the right by $\\mathbf{v}_i$, we get: \n",
    "\\begin{eqnarray}\n",
    "  \\mathbf{v}_j^T \\mathbf{A} \\mathbf{v}_i =  \\mathbf{v}_j^T \\lambda_i\\mathbf{v}_i \\\\\n",
    " \\left(\\mathbf{A} \\mathbf{v}_j\\right)^T\\mathbf{v}_i = \\left(\\lambda_j\\mathbf{v}_j\\right)^T\\mathbf{v}_i\\\\\n",
    "   \\mathbf{v}_j^T\\mathbf{A}\\mathbf{v}_i = \\lambda_j\\mathbf{v}_j^T\\mathbf{v}_i\\\\\n",
    "   \\mathbf{v}_j^T \\mathbf{A} \\mathbf{v}_i - \\mathbf{v}_j^T \\mathbf{A} \\mathbf{v}_i = 0 = \\left(\\lambda_i - \\lambda_j\\right)\\mathbf{v}_j^T\\mathbf{v}_i\n",
    "\\end{eqnarray}\n",
    "\n",
    "Thus, $\\mathbf{v}_j^T\\mathbf{v}_i = 0$. \n",
    "\n",
    "* The eigenvectors can be interpreted as an orthogonal axis defined by the data. \n",
    "* Since all data is noisy, we can concentrate on the axes corresponding to the largest eigenvectors if you are interested in preserving the variance of the data.  \n",
    "* However, when using PCA within a classification problem, it is much more difficult because we are interested in discriminability (not necessarily variance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Analysis - Maximal Variance Formulation\n",
    "\n",
    "* PCA is a linear transformation\n",
    "* PCA minimizes the redundancy of the resulting transformed data (by ending up data that is uncorrelated), minimizes the mean squared error between original and transformed/reduced data, and maximizes the retained variance of the data. \n",
    "\n",
    "* Consider a data set of observations $\\left\\{ \\mathbf{x}_n \\right\\}_{n=1}^N$ and $\\mathbf{x}_n \\in \\mathbb{R}^D$. We want to maximize the variance of the projected data. \n",
    "\n",
    "* Let us first consider reducing dimensionality to $M = 1$.  Let us define the projection as a vector $\\mathbf{u}_1$ where $\\mathbf{u}_1^T\\mathbf{u}_1 = 1$.  Then, each projected data point into 1-D would be $y_n = \\mathbf{u}_1^T\\mathbf{x}_n$\n",
    "\n",
    "* The mean of the sample data is $\\bar{\\mathbf{x}} = \\frac{1}{N}\\sum_{n=1}^N\\mathbf{x}_n$ and the mean of the projected data is $\\mathbf{u}_1^T\\bar{\\mathbf{x}}$\n",
    "\n",
    "* The variance of projected data is: \n",
    "\\begin{eqnarray}\n",
    "\\frac{1}{N} \\sum_{n=1}^N \\left\\{ \\mathbf{u}_1^T\\mathbf{x}_n - \\mathbf{u}_1^T\\bar{\\mathbf{x}} \\right\\}^2 & = & \\frac{1}{N} \\sum_{n=1}^N  \\left( \\mathbf{u}_1^T\\mathbf{x}_n - \\mathbf{u}_1^T\\bar{\\mathbf{x}} \\right)\\left( \\mathbf{u}_1^T\\mathbf{x}_n - \\mathbf{u}_1^T\\bar{\\mathbf{x}} \\right)^T\\\\\n",
    "& = & \\frac{1}{N} \\sum_{n=1}^N  \\left( \\mathbf{u}_1^T\\mathbf{x}_n - \\mathbf{u}_1^T\\bar{\\mathbf{x}} \\right) \\left( \\mathbf{x}_n^T\\mathbf{u}_1 - \\bar{\\mathbf{x}}^T\\mathbf{u}_1 \\right)\\\\\n",
    "& = & \\frac{1}{N} \\sum_{n=1}^N  \\mathbf{u}_1^T\\mathbf{x}_n\\mathbf{x}_n^T\\mathbf{u}_1 - \\mathbf{u}_1^T\\mathbf{x}_n\\bar{\\mathbf{x}}^T\\mathbf{u}_1 - \\mathbf{u}_1^T\\bar{\\mathbf{x}}\\mathbf{x}_n^T\\mathbf{u}_1 + \\mathbf{u}_1^T\\bar{\\mathbf{x}}\\bar{\\mathbf{x}}^T\\mathbf{u}_1\\\\\n",
    "& = & \\mathbf{u}_1^T \\left( \\frac{1}{N} \\sum_{n=1}^N \\mathbf{x}_n\\mathbf{x}_n^T - \\mathbf{x}_n\\bar{\\mathbf{x}}^T - \\bar{\\mathbf{x}}\\mathbf{x}_n^T+ \\bar{\\mathbf{x}}\\bar{\\mathbf{x}}^T\\right)\\mathbf{u}_1\\\\\n",
    "& = & \\mathbf{u}_1^T  \\left( \\frac{1}{N} \\sum_{n=1}^N  \\left(\\mathbf{x}_n - \\bar{\\mathbf{x}}\\right)\\left(\\mathbf{x}_n - \\bar{\\mathbf{x}}\\right)^T \\right) \\mathbf{u}_1\\\\\n",
    "& = & \\mathbf{u}_1^T\\mathbf{S}\\mathbf{u_1}\n",
    "\\end{eqnarray}\n",
    "\n",
    "* Now, we can maximize the projected variance with respect to $\\mathbf{u}_1$ while constraining $\\mathbf{u}_1^T\\mathbf{u}_1 = 1$.  We will do this using a Lagrange multiplier: \n",
    "\\begin{equation}\n",
    "L = \\mathbf{u}_1^T\\mathbf{S}\\mathbf{u_1} + \\lambda_1\\left(1 - \\mathbf{u}_1^T\\mathbf{u}_1\\right)\n",
    "\\end{equation}\n",
    "\n",
    "* By taking the derivative of the Lagrangian and setting it equal to zero, we get: \n",
    "\\begin{equation}\n",
    "\\mathbf{S}\\mathbf{u}_1 = \\lambda_1\\mathbf{u_1}\n",
    "\\end{equation}\n",
    "\n",
    "* We can multiply the left side by $\\mathbf{u}_1^T$ and get:\n",
    "\t\\begin{equation}\n",
    "\t\\mathbf{u_1}^T\\mathbf{S}\\mathbf{u_1} = \\lambda_1\n",
    "\t\\end{equation}\n",
    "\n",
    "* So the variance of the projected data is equal to the eigenvalue of the covariance matrix of the sample data along the direction of the eigenvector used for dimensionality reduction. \n",
    "* We can incrementally add new eigenvector directions (ordered by maximal eigenvalue/variance) to project into an $M$ dimensional space where $1\\leq M \\leq D$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for Minimization of Mean Squared Error\n",
    "\n",
    "* We can also look at PCA as a minimization of mean squared error.  \n",
    "* Consider $\\mathbf{x}\\in R^n$ and an orthogonal basis $\\mathbf{a}$:\n",
    "\n",
    " \\begin{equation}\n",
    " \\hat{\\mathbf{x}} = \\sum_{i=1}^m y_i\\mathbf{a}_i\n",
    " \\end{equation}\n",
    " where $m < n$.  \n",
    " \\begin{equation}\n",
    " y_j = \\mathbf{x}^T\\mathbf{a}_j\n",
    " \\end{equation} where $\\mathbf{A}^T\\mathbf{A}=\\mathbf{I}$\n",
    "\n",
    "We want to minimize the residual error: \n",
    "\\begin{equation}\n",
    "\\epsilon = \\mathbf{x} - \\hat{\\mathbf{x}} = \\sum_{j=m+1}^n y_j \\mathbf{a}_j\n",
    "\\end{equation}\n",
    "The objective we will use is the mean square residual:\n",
    "\\begin{eqnarray}\n",
    "J &=& E\\{ \\|\\epsilon\\|^2_2\\}\\\\\n",
    "&=& E\\left\\{\\left( \\sum_{i=m+1}^n y_i \\mathbf{a}_i^T\\right)\\left( \\sum_{i=m+1}^n y_i \\mathbf{a}_i\\right) \\right\\}\\\\\n",
    "&=&\\sum_{j=m+1}^n E \\{y_j^2\\}\\\\\n",
    "&=&\\sum_{j=m+1}^n E \\{(\\mathbf{a}_j^T\\mathbf{x})(\\mathbf{x}^T\\mathbf{a}_j)\\}\\\\\n",
    "&=& \\sum_{j=m+1}^n \\mathbf{a}_j^T E\\{\\mathbf{x}\\mathbf{x}^T\\}\\mathbf{a}_j\\\\\n",
    "&=& \\sum_{j=m+1}^n \\mathbf{a}_j^T R_x\\mathbf{a}_j\n",
    "\\end{eqnarray}\n",
    "Minimize the error and incorporate Lagrange parameters for $\\mathbf{A}^T\\mathbf{A}=\\mathbf{I}$:\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial J}{\\partial \\mathbf{a}_j} &=& 2(R_x\\mathbf{a}_j - \\lambda_j\\mathbf{a}_j) = 0 \\text{ for }j = m+1 \\ldots n\\\\\n",
    "R_x\\mathbf{a}_j &=& \\lambda_j\\mathbf{a}_j\n",
    "\\end{eqnarray}\n",
    "So, the sum of the error is the sum of the eigenvalues of the unused eigenvectors.  So, we want to select the eigenvectors with the $m$ largest values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Assignment\n",
    "\n",
    "* Section 1.4 \n",
    "* Section 12.1\n",
    "* Appendix C\n",
    "* (and Appendix E for Lagrange Optimization discussed in the previous lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
