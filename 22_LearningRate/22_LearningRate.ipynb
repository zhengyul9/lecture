{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 24 - Adaptive Learning Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in a Nutshell\n",
    "\n",
    "Suppose you have the data $\\{x_i\\}_{i=1}^N \\in \\mathbb{R}^D$ with labels $\\{d_i\\}_{i=1}^N \\in \\mathbb{R}^K$. You want to find a mapper that learns the input samples $x_i$ and maps it to a label response $d_i$, i.e. *classification*. \n",
    "\n",
    "Consider the following objective function:\n",
    "\n",
    "$$J(w) = \\frac{1}{2} \\sum_{j=1}^p e_p^2 = \\frac{1}{2} \\sum_{j=1}^p \\left(d_p-y_p\\right)^2$$\n",
    "\n",
    "where $p$ is the dimensionality of your desired values.\n",
    "\n",
    "1. **Forward Pass:**\n",
    "For any neuron $i$ receiving input signals from neurons $j$:\n",
    "\n",
    "$$y_i = \\phi\\left(\\sum_{j=1}^M w_{ij}x_j\\right)$$\n",
    "\n",
    "where $\\phi(\\bullet)$ is a pre-defined activation function. (The weights and biases of the network have been initialized to some random value.)\n",
    "\n",
    "2. **Backward Pass:** Compute the gradient by defining local errors at every layer and neuron\n",
    "\n",
    "$$\\delta_i = \\phi'(v_i)\\sum_j \\delta_j w_{ij}$$\n",
    "\n",
    "$$\\Delta w_{ij} = - \\eta \\delta_j y_i$$\n",
    "\n",
    "and update the parameters:\n",
    "\n",
    "$$w_{ij}^{(n+1)} = w_{ij}^{(n)} + \\Delta w_{ij}$$\n",
    "\n",
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Linear Systems\n",
    "\n",
    "In regression, we found an analytic solution for the *optimal* weights of our regressor.\n",
    "\n",
    "For some data $\\{x_i\\}_{i=1}^N$ and labels $\\{d_i\\}_{i=1}^N$, we found that the optimal weights are:\n",
    "\n",
    "$$w^* = (X^TX)^{-1} X^Td$$\n",
    "\n",
    "using the mean squared error function $J = \\frac{1}{2N} \\sum_{i=1}^N (d_i - wx_i)^2$\n",
    "\n",
    "Note that if we were operating in the feature space of $X$, $\\phi(X)$, the optimal weights are: \n",
    "\n",
    "$$w^* = (\\phi(X)^T\\phi(X))^{-1} \\phi(X)^Td$$\n",
    "\n",
    "Assuming, the data $X$ is demeaned, $R = X^TX$ is called the *covariance* of the input data and $P = X^Td$ is called the *cross-covariance* of the input data with the desired signal.\n",
    "\n",
    "Such analytic solution only exists if the model is a *linear function on the parameters*, i.e., $y = Xw$ or $y = \\phi(X) w$, which is not the case for an MLP output!\n",
    "\n",
    "* We can find a solution by performing a *search* of the performance surface (governed by the error function $J$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Error Surface for a Linear Neuron\n",
    "\n",
    "* The mean squared error function $J = \\frac{1}{2N} \\sum_{i=1}^N e_i^2$ is a *convex* function and therefore we can apply convex optimization techniques to search for the *minima* of this function.\n",
    "    * The most common method to optimize the least squares error is the *steepest descent*.\n",
    "\n",
    "* The error surface lies in a space with a horizontal axis for each weight and one vertical axis for the error.\n",
    "    * For a linear neuron with a squared error, it is a quadratic bowl.\n",
    "    * Vertical cross-sections are parabolas.\n",
    "    * Horizontal cross-sections are ellipses.\n",
    "    \n",
    "* For multi-layer, non-linear nets the error surface is much more complicated.\n",
    "    * But locally, a piece of a quadratic bowl is usually a very good apparoximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "## Convergence Speed of Full-Batch Learning\n",
    "\n",
    "* Going \"downhill\" reduces the error, but the direction of steepest descent does not point at the minimum unless the ellipse is a circle.\n",
    "    * The gradient is big in the direction in which only want to travel a small distance.\n",
    "    * The gradient is small in the direction in which we want to travel a large distance\n",
    "    \n",
    "* Even for non-linear multi-layer nets, the error surface is locally quadratic, so the same speed issues apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "## Learning Rate\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\eta \\nabla J(w^{(t)})$$\n",
    "\n",
    "* If the learning rate is big, the weights slosh to and from the ravine.\n",
    "    * If the learning rate is too big, this oscillation diverges.\n",
    "    \n",
    "* What we would like to achieve:\n",
    "    * Move quickly in direction with small but consistent gradients.\n",
    "    * Move slowly in directions with big but inconsistent gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "## Stochastic Gradient Descent - Online Learning\n",
    "\n",
    "* If the dataset is redundant, the gradient on the first half is almost identical to the gradient on the second half.\n",
    "    * So instead of computing the ful gradient, update the weights using the gradient on the first half and then get a gradient for the new weights on the second half.\n",
    "    * The extreme version of this approach updates weights after each point. It's called **online learning**.\n",
    "    \n",
    "* Mini-batches are usually better than online.\n",
    "    * Less computation is used for updating the weights.\n",
    "    * Computing the gradient for many cases simultaneously uses matrix-matrix multiplications which are very efficient, especially with GPUs.\n",
    "\n",
    "* Mini-batches need to be balanced for classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "## Full-Batch vs Mini-Batch Learning\n",
    "\n",
    "* If we use the full gradient computed from all the training cases, **batch learning**, there are many clever ways to speed up (e.g. non-linear conjugate gradient).\n",
    "    * The optimization community has studied the general problem of optimizing smooth non-linear functions for many years.\n",
    "   * Multilayer neural nets are not typical of the problems they study so their methods may need a lot of modifications.\n",
    "   \n",
    "* For large neural networks with very large and highly redundant training sets, it is nearly always best to use **mini-batch learning**.\n",
    "    * The mini-batches may need to be quite big when adapting fancy methods.\n",
    "    * Big mini-batches are more computationaly efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Batch Gradient Descent\n",
    "\n",
    "* We start by guessing a learning rate.\n",
    "    * If the error keeps getting worse or oscillates wildly, reduce the learning rate.\n",
    "    * If the error is falling fairly consistently but slowly, increase the learning rate.\n",
    "\n",
    "* We can write a simple routine to automate this way of adjusting the learning rate.\n",
    "\n",
    "* Towards the end of mini-batch learning it nearly always helps to turn down the learning rate.\n",
    "    * This removes fluctuations in the final weights caused by the variations between mini-batches.\n",
    "\n",
    "* Turn down the learning rate when the error stops decreasing.\n",
    "    * Use the error on a separate validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Bag of Tricks for Mini-Batch Gradient Descent\n",
    "\n",
    "<span style=\"color:blue\">**Initializing the Weights**</span>\n",
    "\n",
    "* If two hidden units have exactly the same bias and exactly the same incoming and outgoing weights, they will always get exactly the same gradient.\n",
    "    * So they can never learn to be different features.\n",
    "    * We break symmetry by initializing the weights to have small random values.\n",
    "    \n",
    "* If a hidden unit has a big *fan-in*, small changes on many of its incoming weights can cause the learning to overshoot.\n",
    "    * We generally want smaller incoming weights when the fan-in is big, so initialize the weights to be proportional to $\\sqrt{\\text{fan-in}}$.\n",
    "    \n",
    "<span style=\"color:blue\">**Shifting the Input (Demeaning)**</span>\n",
    "* When using the steepest descent, shifting the input values makes a big difference!\n",
    "    * It usually helps to transform each component of the input vector so that it has zero mean over the whole training set.\n",
    "* The hyperbolic tangent produces hidden activations that are roughly zero mean.\n",
    "    * In this repect it's better than the logistic\n",
    "    \n",
    "<span style=\"color:blue\">**Scalling the Input (Unit Variance)**</span>\n",
    "* When using steepest descent, scaling then input values makes a big difference.\n",
    "    * It usually helps to transform each component of the input vector over the whole training set.\n",
    "\n",
    "<span style=\"color:blue\">**Decorrelate the Input components**</span>\n",
    "* For a linear neuron, we get a big win by decorrelating each component of the input from the other input components.\n",
    "\n",
    "* There are several different ways to decorrelate inputs. A reasonable method is to use *Principal Component Analysis (PCA)*.\n",
    "    * Drop the principal components with the smallest eigenvalues.\n",
    "        * This achieves some dimensionality reduction.\n",
    "    * Divide the remaining principal components by the square roots of their eigenvalues. For a linear neuron, this converts an axis aligned elliptical error surface into a circular one.\n",
    "\n",
    "* For a circular error surface, the gradient points straight towards the minimum.\n",
    "\n",
    "<span style=\"color:orange\">**Common Problems**</span>\n",
    "\n",
    "**Plateau**\n",
    "* If we start with a very big learning rate, the weights of each hidden unit will all bcome very big and positive or very big and negative.\n",
    "    * The error derivatives for the hidden units will all become tiny and the error will not decrease.\n",
    "    * This is usualy a *plateau*, but can be misunderstood as a local minimum.\n",
    "\n",
    "**Turning Down Learning Rate**\n",
    "* Turning down the learning rate reduces the random fluctuations in the error due to the different gradients on different mini-batches.\n",
    "    * So we get a quick win.\n",
    "    * But then we get slower learning.\n",
    "* So, don't turn down the learning rate too soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Training Networks\n",
    "\n",
    "Here are a few ways to speed up mini-batch learning:\n",
    "\n",
    "**Momentum**\n",
    "* Instead of using the gradient to change *position* of the weight *particle*, use it to change the *velocity*.\n",
    "\n",
    "**Adaptive Learning Rate**\n",
    "* Use separate adaptive learning rates for each parameter.\n",
    "* Slowly adjust the rate using the consistency of the gradient for that parameter.\n",
    "\n",
    "**RMSProp (Root Mean Square Propagation)**\n",
    "* Use separate adaptive learning rates for each parameter.\n",
    "* Divide the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight.\n",
    "    * This is the mini-batch version of just using the sign of the gradient (method called **RProp** designed for full-batch learning).\n",
    "\n",
    "**ADAM (Adaptive Moment Estimation)**\n",
    "* Use separate adaptive learning rates for each parameter.\n",
    "* Adaptation of RMSProp, running averages of both gradient and second moments are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Momentum Learning\n",
    "\n",
    "* The momentum learning can be applied to full-batch learning or mini-batch learning.\n",
    "    * Probably the commonest recipe to learn deep neural nets is to use stochastic gradient descent with mini-batches combined with momentum.\n",
    "    \n",
    "* Imagine a ball on the error surface. The location of the ball in the horizontal plane represents the weight vector.\n",
    "    * The ball starts off by following the gradient, but once it has velocity, it no longer does steepest descent.\n",
    "    * Its momentum makes it keep going in the previous direction.\n",
    "    * We need to introduce *viscosity* so that the velocity dies off when are getting closer to the solution.\n",
    "\n",
    "* It damps oscillations in directions of high curvature by combining gradients with opposite signs.\n",
    "\n",
    "* It builds up speed in directions with a gentle but consistent gradient.\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta w_{ji}^{(t)} &= \\alpha \\Delta w_{ji}^{(t-1)} + \\eta  \\nabla J(w_{ji}^{(t)})\\\\\n",
    "&= \\alpha \\Delta w_{ji}^{(t-1)} + \\eta \\delta_j^{(t)} y_i^{(t)}\n",
    "\\end{align}\n",
    "\n",
    "The effect of the gradient is to increment the previous velocity. The velocity also decays by $\\alpha$ which is slightly less than 1 (generally, $\\alpha=0.9$).\n",
    "\n",
    "* At the beginning of learning there may be very large gradients.\n",
    "    * So it pays off to use a small momentum (e.g. $\\alpha=0.5$).\n",
    "    * Once the large gradients have disappeared and the weights are stuck in a ravine the momentum can be smoothly raised to its final value (e.g. $\\alpha=0.9$ or even $0.99$).\n",
    "    \n",
    "* This allows us to learn at a rate that would have caused divergent oscillations without momentum (case of increased learning rate only).\n",
    "\n",
    "## Nesterov's Accelerated Gradient Descent\n",
    "\n",
    "* The standard momentum *first* computes the gradient at the current location and *then* takes a big jump in the direction of the updated accumulated gradient.\n",
    "\n",
    "* Ilya Sutskever (2012 unpublished) suggested a new form of momentum that often works better.\n",
    "    * Inspired by the Nesterov method for optimizing convex functions.\n",
    "* *First* make a big jump in the direction of the previous accumulated gradient.\n",
    "\n",
    "* *Then* measure the gradient where you end up and make a correction.\n",
    "\n",
    "$$\\Delta w_{ji}^{(t)} = \\alpha \\Delta w_{ji}^{(t-1)} + \\eta \\nabla J(w_{ji}^{(t-1)} + \\eta \\Delta w_{ji}^{(t-1)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (General) Adaptive Learning Rate\n",
    "\n",
    "* In multilayer neural networks, the appropriate learning rates can vary widely between weights:\n",
    "    * The magnitude of the gradient are often very different for different layers, especially if the initial weights are small.\n",
    "    * The fan-in of a unit determines the size of the \"overshoot\" effects caused by simultaneously changing many of the incoming weights of a unit to correct the same error.\n",
    "\n",
    "* So use a global learning rate multiplied by an appropriate local gain that is determined empirically for each weight.\n",
    "\n",
    "* Start with a local gain of 1 for every weight.\n",
    "\n",
    "* Increase the local gain if the gradient for that weight does not change sign.\n",
    "\n",
    "* Use small additive increases and multiplicative decreases.\n",
    "    * This ensures that big gains decay rapidly when oscillations start.\n",
    "    * If the gradient is totally random the gain will hover around 1 when we increase by plus $\\delta$ half the time and decrease by times $1-\\delta$ half the time.\n",
    "    \n",
    "$$\\Delta w_{ji} = -\\eta g_{ji} \\nabla J(w_{ji})$$\n",
    "\n",
    "\\begin{align}\n",
    "\\text{If } &\\left(\\nabla J(w_{ji}^{(t)}) \\times \\nabla J(w_{ji}^{(t-1)})\\right) \\geq 0 \\\\\n",
    "&\\text{then } g_{ji}(t) = g_{ji}(t-1) + \\delta \\\\\n",
    "&\\text{else } g_{ji}(t) = g_{ji}(t-1) \\times \\delta \\\\\n",
    "\\end{align}\n",
    "\n",
    "* Need to limit the gains to lie in some reasonable range, e.g. $[0.1,10]$ or $[0.01,100]$.\n",
    "\n",
    "* Use full batch learning or very large mini-batches.\n",
    "    * This ensures that changes in the sign of the gradient are not mainly due to the sampling error of a mini-batch.\n",
    "    \n",
    "* Adaptive learning rates can be combined with momentum (Jacobs, 1989)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RProp\n",
    "\n",
    "* RProp stands for *Resilient BackPropagation*.\n",
    "\n",
    "* The magnitude of the gradient can be very different for different weights and can change during learning.\n",
    "    * This make it hard to choose a single glocal learning rate.\n",
    "    \n",
    "* For full-batch learning, we can deal with this variation by only using the sign of the gradient.\n",
    "    * The weight updates are all of the same magnitude.\n",
    "    * This escapes from plateaus with tiny gradients quickly.\n",
    "\n",
    "$$\\Delta w_{ji} = -\\eta g_{ji}\\text{sign}\\left( \\nabla J(w_{ji})\\right)$$\n",
    "\n",
    "\\begin{align}\n",
    "\\text{If } &\\left(\\nabla J(w_{ji}^{(t)}) \\times \\nabla J(w_{ji}^{(t-1)})\\right) \\geq 0 \\\\ \n",
    "& \\text{then } g_{ji}(t) = g_{ji}(t-1) \n",
    "\\times \\delta_1 \\\\\n",
    "& \\text{else } g_{ji}(t) = g_{ji}(t-1) \\times \\delta_2\n",
    "\\end{align}\n",
    "\n",
    "* RProp combines the idea of only using the sign of the gradient with the idea of adapting the learning rate separately for each weight.\n",
    "    * Increase the learning rate for a weight multiplicatively if the signs of its last two gradients agree.\n",
    "    * Otherwise decrease the step size multiplicatively.\n",
    "\n",
    "* Use full batch learning or very big mini-batches.\n",
    "   * This ensures that changes in the sign of the gradient are not mainly due to the sampling error of a mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp\n",
    "\n",
    "* RProp is equivalent to using the gradient but also dividing by the size of the gradient.\n",
    "    * The problem with mini-batch RProp is that we divide by a different number for each mini-batch.\n",
    "\n",
    "* RMSProp keeps a moving average of the squared gradient for each weight.\n",
    "\n",
    "$$\\Delta w_{ji}^{(t)} = \\gamma \\Delta w_{ji}^{(t-1)} + (1-\\gamma) \\left(\\nabla J(w_{ji}^{(t)})\\right)^2$$\n",
    "\n",
    "$$w_{ji}^{(t+1)} = w_{ji}^{(t)} - \\frac{\\eta}{\\sqrt{\\Delta w_{ji}^{(t)}}} \\nabla J(w_{ji}^{(t)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAM\n",
    "\n",
    "* ADAM is a combination of RMSProp and momentum.\n",
    "\n",
    "* ADAM keeps both moving average of the gradient and the squared gradient.\n",
    "\n",
    "* Adam includes bias corrections to the estimates of both the Ô¨Årst-order moments (the momentum term) and the (uncentered) second-order moments to account for their initializationat the origin.\n",
    "    * Thus, unlike in ADAM, the RMSProp second-order moment estimate may have high biasearly in training. \n",
    "\n",
    "$$m_w^{(t+1)} = \\beta_1 m_w^{(t)} + (1-\\beta_1)\\nabla J(w^{(t)})$$\n",
    "\n",
    "$$v_w^{(t+1)} = \\beta_2 v_w^{(t)} + (1-\\beta_2)\\left(\\nabla J(w^{(t)})\\right)^2$$\n",
    "\n",
    "$$\\hat{m}_w = \\frac{m_w^{(t+1)}}{1 - \\beta_1^{t+1}}\\text{ (bias correction)}$$\n",
    "\n",
    "$$\\hat{v}_w = \\frac{v_w^{(t+1)}}{1 - \\beta_2^{t+1}}\\text{ (bias correction)}$$\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\eta \\frac{\\hat{m}_w}{\\sqrt{\\hat{v}_m} + \\epsilon}$$\n",
    "\n",
    "* Often used $\\beta_1 = 0.9$, $\\beta_2 = 0.999$ and $\\epsilon = 10^{-8}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Learning Methods for Neural Networks\n",
    "\n",
    "* For small datasets (e.g. 10,000 samples) or bigger datasets without much redundacy, use a full-batch method.\n",
    "    * AdaGrad, RProp, ...\n",
    "    \n",
    "* For big, redundant datasets use mini-batches.\n",
    "    * Try gradient descent with momentum.\n",
    "    * Try RMSProp.\n",
    "    * Try ADAM.\n",
    "\n",
    "*Why there is no simple recipe?*\n",
    "\n",
    "* There are lots of different network architectures\n",
    "* Tasks differ a lot\n",
    "    * Some require very accurate weights, some don't.\n",
    "    * Some have many very rare cases (e.g. words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Reading\n",
    "\n",
    "Chapter 8 \"Optimization for Training Deep Models\" from Deep Learning by Ian Goodfellow\n",
    "* http://www.deeplearningbook.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
