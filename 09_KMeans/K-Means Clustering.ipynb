{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering algorithm \n",
    "\n",
    "* *What is clustering?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The K-means algorithm is summarized as: \n",
    "     * Set number of clusters, M\n",
    "     * Initialize cluster centers\n",
    "     * Do until Change in cluster centers is small:\n",
    "         * FOR i = 1 to N\n",
    "             * Determine the closest representative, $\\Theta_j$, for $\\mathbf{x}_i$\n",
    "             * Set label for data point $i$ to $j$\n",
    "         * FOR j = 1 to M\n",
    "             * Update cluster representative $\\Theta_j$ to the mean of the points with cluster label $j$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The objective function for the K-means clustering algorithm is\n",
    "\\begin{equation}\n",
    "J(\\Theta, U) = \\sum_{i=1}^N \\sum_{j=1}^m u_{ij}\\left\\| \\mathbf{x}_i - \\theta_j \\right\\|^2\n",
    "\\end{equation}\n",
    "where $u_{ij} \\in \\{0,1\\}$ and is '1' for the $j$ index corresponding to the class label assigned to data point $\\mathbf{x}_i$ and zero otherwise. The $\\theta_j$ vector is the $j^{th}$ cluster representative.\n",
    "* *How would you optimize this objective function?*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Does the K-means algorithm find the *globally optimal* solution (i.e., the cluster centers and assignments that globally minimize the objective function)?  \n",
    "* Does the K-means algorithm make any assumptions on cluster shape? \n",
    "* Given a data set with an unknown number of clusters, come up with a strategy for determining the \"right\" number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets \n",
    "from scipy import spatial\n",
    "%matplotlib inline\n",
    "\n",
    "def KMeans(X, C):\n",
    "    MaxIter     = 10000\n",
    "    StopThresh  = 1e-5\n",
    "\n",
    "    #Initialize Cluster Centers by drawing randomly from input data (can use other\n",
    "    # methods for initialization...)\n",
    "    N       = X.shape[0] #number of data points\n",
    "    d       = X.shape[1] #dimensionality\n",
    "    rp      = np.random.permutation(N) #random permutation of numbers 1:N\n",
    "    centers = X[rp[0:C],:] #select first M data points sorted according to rp\n",
    "\n",
    "    diff    = 1e100\n",
    "    iter    = 0\n",
    "    while((diff > StopThresh) & (iter < MaxIter)):\n",
    "        #Assign data to closest cluster representative (using Euclidean distance)\n",
    "        D   = spatial.distance.cdist(X, centers)\n",
    "        L   = np.argmin(D, axis=1)\n",
    "        \n",
    "        #Update cluster centers\n",
    "        centersPrev = centers.copy()\n",
    "        for i in range(C):\n",
    "            centers[i,:] = np.mean(X[L == i,:], axis=0)\n",
    "\n",
    "        #Update diff & iteration count for stopping criteria\n",
    "        diff = np.linalg.norm(centersPrev - centers)\n",
    "        iter = iter+1\n",
    "    return centers, L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1500\n",
    "n_clusters = 3\n",
    "\n",
    "# Make Blob Data\n",
    "X, y_blobs = datasets.make_blobs(n_samples=n_samples)\n",
    "\n",
    "#Cluster\n",
    "centers, L = KMeans(X,n_clusters)\n",
    "\n",
    "#Plot Results\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(221)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_blobs)\n",
    "plt.title(\"Blobs with True Labels\")\n",
    "plt.subplot(222)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=L)\n",
    "plt.title(\"Clustered Blobs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anisotropicly distributed data\n",
    "#some examples from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html\n",
    "\n",
    "n_samples = 1500\n",
    "n_clusters = 3\n",
    "\n",
    "#generate data\n",
    "transformation = [[ 0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "X, y = datasets.make_blobs(n_samples=n_samples)\n",
    "X = np.dot(X, transformation)\n",
    "\n",
    "#cluster data\n",
    "centers, L = KMeans(X,n_clusters)\n",
    "\n",
    "#plot data\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(221)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title(\"Data with True Labels\")\n",
    "plt.subplot(222)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=L)\n",
    "plt.title(\"Clustered Results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with different variances\n",
    "\n",
    "n_samples = 1500\n",
    "n_clusters = 3\n",
    "\n",
    "#generate data\n",
    "X, y = datasets.make_blobs(n_samples=n_samples,cluster_std=[1.0, 2.5, 0.5])\n",
    "\n",
    "#cluster data\n",
    "centers, L = KMeans(X,n_clusters)\n",
    "\n",
    "#plot data\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(221)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title(\"Data with True Labels\")\n",
    "plt.subplot(222)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=L)\n",
    "plt.title(\"Clustered Results\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uneven sized blobs\n",
    "\n",
    "n_samples = 1500\n",
    "n_clusters = 3\n",
    "\n",
    "#generate data\n",
    "X, y = datasets.make_blobs(n_samples=n_samples)\n",
    "X = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10]))\n",
    "y = np.hstack((np.ones(500), 2*np.ones(100), 3*np.ones(10)))\n",
    "              \n",
    "#cluster data\n",
    "centers, L = KMeans(X,n_clusters)\n",
    "\n",
    "#plot data\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(221)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title(\"Data with True Labels\")\n",
    "plt.subplot(222)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=L)\n",
    "plt.title(\"Clustered Results\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moons\n",
    "\n",
    "n_samples = 1500\n",
    "n_clusters = 3\n",
    "\n",
    "#generate data\n",
    "X, y = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "\n",
    "#cluster data\n",
    "centers, L = KMeans(X,n_clusters)\n",
    "\n",
    "#plot data\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(221)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title(\"Data with True Labels\")\n",
    "plt.subplot(222)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=L)\n",
    "plt.title(\"Clustered Results\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circles\n",
    "\n",
    "n_samples = 1500\n",
    "n_clusters = 3\n",
    "\n",
    "#generate data\n",
    "X, y = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)\n",
    "\n",
    "#cluster data\n",
    "centers, L = KMeans(X,n_clusters)\n",
    "\n",
    "#plot data\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(221)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title(\"Data with True Labels\")\n",
    "plt.subplot(222)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=L)\n",
    "plt.title(\"Clustered Results\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Validity\n",
    "\n",
    "* *How would you evaluate clustering results?*  As discussed and illustrated in our first lecture, clustering results can be subjective and/or the desired result can be application dependent. One approach is the use of *cluster validity indices*\n",
    "\n",
    "* There are many cluster validity indices in the literature. \n",
    "\n",
    "* Cluster validity measures are used for a number of different goals.  For example, cluster validity metrics can be used to compare clustering results, try to determine the *correct* number of clusters, try to select the *correct* parameter settings, try to evaluate the appropriateness of the clustering result based on the data only (and not using another result or \"ground truth\" data). \n",
    "\n",
    "\n",
    "* *External index* is used to measure how well a clustering result matches a set of supplied class labels.  This can be used to compare to \"ground truth\" as well as to compare different clustering results to see how similar they are (and how stable a particular clustering is on a data set across parameter settings and/or algorithms).  An example of an external index is the Rand index: \n",
    "\n",
    "Given a set of $n$ data points, $\\mathbf{X} = \\{x_1, \\ldots, x_n\\}$ and two partitions (i.e., clustering results) of $\\mathbf{X}$ to compare, $C = \\{C_1, \\ldots, C_r\\}$, a partition of $\\mathbf{X}$ into $r$ partitions, and $D = \\{D_1, \\ldots, D_s\\}$, a partition of $\\mathbf{X}$ into $s$ partitions, define the following:\n",
    "* $a$, the number of pairs of elements in $X$ that are in the same subset in $C$ and in the same subset in $D$\n",
    "* $b$, the number of pairs of elements in  $X$  that are in different subsets in $C$  and in different subsets in $D$\n",
    "* $c$, the number of pairs of elements in $X$ that are in the same subset in $C$  and in different subsets in $D$\n",
    "* $d$, the number of pairs of elements in  $X$  that are in different subsets in $C$  and in the same subset in $D$\n",
    "\n",
    "The Rand index, $R$, is:\n",
    "$R = \\frac{a+b}{a+b+c+d}$\n",
    "Intuitively,  $a + b$ can be considered as the number of agreements between $C$ and $D$ and $c + d$ as the number of disagreements between $C$ and $D$. The numerator is the number of agreements and the denominator is the total number of pairs (agreements and disagreements)\n",
    "\n",
    "* There are many other external cluster validity indices, the Rand index is just one example!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An *Internal index* is used to measure how well a clustering result is without using any external labels or other results. \n",
    "\n",
    "* Many internal indices are based on measuring within-cluster vs. between-cluster variation with the idea being within cluster variation should be small and between cluster variation should be large.  \n",
    "\n",
    "* One example of an internal index is the Dunn's index: \n",
    "\n",
    "\n",
    "Let $C_i$ be a cluster of vectors. Let $x$ and $y$ be any two $n$ dimensional feature vectors assigned to the same cluster $C_i$.\n",
    "\n",
    "$\\Delta_i =  \\text{max}_{x,y \\in C_i} d(x,y)$  calculates the maximum distance in a cluster.\n",
    "\n",
    "\n",
    " Let $\\delta(C_i,C_j)$ be the intercluster distance metric, between clusters $C_i$ and $C_j$.   This intercluster distance can be computed in a number of ways.  For example, the minimum distance between any two points in the cluster or, alternatively, the distance between the cluster means.\n",
    "\n",
    "With the above notation, if there are $m$ clusters, then the Dunn Index for the set is defined as:\n",
    "\n",
    "$\\mathit{D}_m = \\frac{ \\underset{ 1 \\leqslant i < j \\leqslant m}{\\text{min}} \\left.\\delta(C_i,C_j)\\right.}{ \\underset{ 1 \\leqslant k \\leqslant m}{\\text{max}} \\left.\\Delta_k\\right.}$\n",
    "\n",
    "Like the intercluster distance, alternative measures for the internal cluster distance can be used.  Some examples are: \n",
    "\n",
    "$\\Delta_i =   \\dfrac{1}{|C_i| (|C_i| - 1)} {\\sum}_{x , y \\in C_i, x \\neq y} d(x,y)$ calculates the mean distance between all pairs in a cluster. \n",
    "\n",
    "$\\Delta_i =   \\dfrac{\\underset{x \\in C_i}{\\sum} d(x,\\mu)}{|C_i|} , \\mu =   \\dfrac{\\underset{x \\in C_i}{\\sum} x}{|C_i|}$ calculates the distance of all the points from the mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
